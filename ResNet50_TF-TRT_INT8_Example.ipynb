{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all three models\n",
    "models = [{'model_path': './artifacts/model1.pb', \n",
    "           'input_node_name': 'tower_0/inference_input:0', \n",
    "           'output_node_name': 'tower_0/inference_output:0', \n",
    "           'n_channels': 3, 'n_height': 128, 'n_width': 128},\n",
    "          {'model_path': './artifacts/model2.pb', \n",
    "           'input_node_name': 'import/input_1:0', \n",
    "           'output_node_name': 'import/dense_2/Sigmoid:0', \n",
    "           'n_channels': 3, 'n_height': 256, 'n_width': 256}, \n",
    "          {'model_path': './artifacts/model3.pb', \n",
    "           'input_node_name': 'tower_0/inference_input:0', \n",
    "           'output_node_name': 'tower_0/inference_output:0', \n",
    "           'n_channels': 3, 'n_height': 256, 'n_width': 256}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a model\n",
    "model_number = 0  # possible values are 0, 1, and 2\n",
    "model = models[model_number]\n",
    "\n",
    "# settings\n",
    "model_path = model['model_path']\n",
    "tftrt_graph_path = './artifacts/model1_tftrt_graph.pb'\n",
    "tftrt_int8_graph_path = './artifacts/model1_tftrt_int8_graph.pb'\n",
    "input_node_name = model['input_node_name']\n",
    "output_node_name = model['output_node_name']\n",
    "n_channels, n_height, n_width = model['n_channels'], model['n_height'], model['n_width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference settings\n",
    "n_inferences = 1000\n",
    "batch_size = 1\n",
    "precision_mode = 'INT8'\n",
    "n_calibration_loops = 500  # n_calibration_loops * batch_size = number_of_total_calibration_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random data\n",
    "data = np.random.randint(low=0, high=255, size=(batch_size, n_height, n_width, n_channels))\n",
    "# warmup_sample = data.astype(np.uint8)\n",
    "warmup_sample = data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with TF frozen graph workflow\n",
    "with tf.Session() as sess:\n",
    "    with tf.gfile.GFile(model_path, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        if model_number == 1:\n",
    "            _ = tf.import_graph_def(graph_def)\n",
    "        else:\n",
    "            _ = tf.import_graph_def(graph_def, name='')\n",
    "        \n",
    "\n",
    "        input_node  = sess.graph.get_tensor_by_name(input_node_name)\n",
    "        output_node = sess.graph.get_tensor_by_name(output_node_name)\n",
    "        \n",
    "        # get input placeholder for phase for model #1\n",
    "        if model_number == 1:\n",
    "            ph_1 = sess.graph.get_tensor_by_name('import/bn_conv1/keras_learning_phase:0')\n",
    "        \n",
    "        # Warmup\n",
    "        if model_number == 1:\n",
    "            _ = sess.run(output_node, feed_dict={ph_1: False, input_node: warmup_sample})\n",
    "        else:\n",
    "            _ = sess.run(output_node, feed_dict={input_node: warmup_sample})\n",
    "        \n",
    "        # Run\n",
    "        timings = []\n",
    "        for _ in range(n_inferences):\n",
    "            if model_number == 1:\n",
    "                start = time.time()\n",
    "                results_tf = sess.run(output_node, feed_dict={ph_1: False, input_node: warmup_sample})\n",
    "                end = time.time()\n",
    "            else:\n",
    "                start = time.time()\n",
    "                results_tf = sess.run(output_node, feed_dict={input_node: warmup_sample})\n",
    "                end = time.time()\n",
    "            timings.append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007463693618774414, 0.006991863250732422, 0.006924867630004883, 0.004235267639160156, 0.003983736038208008, 0.004035472869873047, 0.0040433406829833984, 0.003922224044799805, 0.004025936126708984, 0.0033147335052490234, 0.0034232139587402344, 0.003220081329345703, 0.0032901763916015625, 0.003161907196044922, 0.003353118896484375, 0.0034885406494140625, 0.003193378448486328, 0.0032050609588623047, 0.0031785964965820312, 0.003154277801513672, 0.0031197071075439453, 0.003391742706298828, 0.0032083988189697266, 0.0031747817993164062, 0.003216981887817383, 0.0032777786254882812, 0.0033855438232421875, 0.0033779144287109375, 0.003519296646118164, 0.003249645233154297, 0.0032205581665039062, 0.003200531005859375, 0.0032455921173095703, 0.003210783004760742, 0.0031294822692871094, 0.0031173229217529297, 0.0032477378845214844, 0.003152608871459961, 0.003260374069213867, 0.003182649612426758, 0.003165721893310547, 0.0031249523162841797, 0.0033082962036132812, 0.0032334327697753906, 0.0032205581665039062, 0.0032444000244140625, 0.0032455921173095703, 0.0031800270080566406, 0.0032515525817871094, 0.00341796875, 0.0033483505249023438, 0.003112316131591797, 0.0031833648681640625, 0.0033113956451416016, 0.0034902095794677734, 0.003197193145751953, 0.0031385421752929688, 0.003153562545776367, 0.0031557083129882812, 0.0031130313873291016, 0.003131389617919922, 0.0032351016998291016, 0.0032613277435302734, 0.003124237060546875, 0.0033936500549316406, 0.0031120777130126953, 0.003155946731567383, 0.003186464309692383, 0.003065824508666992, 0.0031244754791259766, 0.003141164779663086, 0.00310516357421875, 0.003076314926147461, 0.0030586719512939453, 0.003053903579711914, 0.0032372474670410156, 0.0033922195434570312, 0.0032575130462646484, 0.003173351287841797, 0.0031447410583496094, 0.003114938735961914, 0.003053903579711914, 0.0032052993774414062, 0.00308990478515625, 0.0031294822692871094, 0.0030875205993652344, 0.0030646324157714844, 0.0031027793884277344, 0.0031371116638183594, 0.003129720687866211, 0.00322723388671875, 0.00312042236328125, 0.0032045841217041016, 0.0030617713928222656, 0.0031528472900390625, 0.0031626224517822266, 0.0031578540802001953, 0.003182649612426758, 0.0032346248626708984, 0.003202676773071289, 0.0031070709228515625, 0.0032079219818115234, 0.0031108856201171875, 0.0031404495239257812, 0.003118753433227539, 0.0031380653381347656, 0.0032083988189697266, 0.0031135082244873047, 0.003131389617919922, 0.003361225128173828, 0.003116130828857422, 0.003121614456176758, 0.0031452178955078125, 0.003099679946899414, 0.0031664371490478516, 0.003145933151245117, 0.0031058788299560547, 0.003281116485595703, 0.0031256675720214844, 0.0030977725982666016, 0.003077268600463867, 0.0031142234802246094, 0.003063678741455078, 0.0030574798583984375, 0.0030460357666015625, 0.0031006336212158203, 0.003116130828857422, 0.0031502246856689453, 0.0031006336212158203, 0.003063678741455078, 0.003180265426635742, 0.003332853317260742, 0.003334522247314453, 0.003642559051513672, 0.0032563209533691406, 0.0032515525817871094, 0.0034301280975341797, 0.003323078155517578, 0.0034635066986083984, 0.0034952163696289062, 0.0033478736877441406, 0.0034220218658447266, 0.0034613609313964844, 0.0032401084899902344, 0.0032472610473632812, 0.0034596920013427734, 0.003505706787109375, 0.0034799575805664062, 0.003361940383911133, 0.0033674240112304688, 0.0032355785369873047, 0.0034744739532470703, 0.003451824188232422, 0.0032384395599365234, 0.0034542083740234375, 0.003505229949951172, 0.0034890174865722656, 0.0034775733947753906, 0.003284931182861328, 0.0032503604888916016, 0.0031647682189941406, 0.0031843185424804688, 0.003316164016723633, 0.0032339096069335938, 0.003435850143432617, 0.0032014846801757812, 0.003358602523803711, 0.0032498836517333984, 0.003160715103149414, 0.0031490325927734375, 0.0031375885009765625, 0.003430604934692383, 0.0032155513763427734, 0.003399372100830078, 0.003458738327026367, 0.0032219886779785156, 0.0033321380615234375, 0.0032796859741210938, 0.003237009048461914, 0.003467559814453125, 0.0034637451171875, 0.003311634063720703, 0.0035042762756347656, 0.003220081329345703, 0.003202199935913086, 0.003172636032104492, 0.003228425979614258, 0.003173351287841797, 0.0031249523162841797, 0.0032584667205810547, 0.0032885074615478516, 0.003221273422241211, 0.0031807422637939453, 0.0033974647521972656, 0.0031871795654296875, 0.0031938552856445312, 0.003199338912963867, 0.003125429153442383, 0.0033330917358398438, 0.0032045841217041016, 0.003217458724975586, 0.0031785964965820312, 0.0031862258911132812, 0.003595113754272461, 0.0033659934997558594, 0.0032248497009277344, 0.0031235218048095703, 0.0031452178955078125, 0.0033261775970458984, 0.003328084945678711, 0.0033338069915771484, 0.003216266632080078, 0.003193378448486328, 0.0032911300659179688, 0.003503084182739258, 0.003339052200317383, 0.003194570541381836, 0.003070354461669922, 0.00334930419921875, 0.0032036304473876953, 0.0031731128692626953, 0.003405332565307617, 0.0032558441162109375, 0.003229379653930664, 0.0032231807708740234, 0.003212451934814453, 0.0031969547271728516, 0.0032134056091308594, 0.0032012462615966797, 0.0031485557556152344, 0.003405332565307617, 0.0032196044921875, 0.0032329559326171875, 0.0034155845642089844, 0.0032346248626708984, 0.003246307373046875, 0.003171682357788086, 0.003392934799194336, 0.003407001495361328, 0.003258943557739258, 0.0032558441162109375, 0.0033500194549560547, 0.003212451934814453, 0.0031976699829101562, 0.00318145751953125, 0.003199338912963867, 0.003285646438598633, 0.0032219886779785156, 0.003330230712890625, 0.0032525062561035156, 0.0032684803009033203, 0.0032215118408203125, 0.003194570541381836, 0.003462553024291992, 0.003321409225463867, 0.003323793411254883, 0.0034122467041015625, 0.0032241344451904297, 0.0033278465270996094, 0.0034537315368652344, 0.003192424774169922, 0.0032227039337158203, 0.0032052993774414062, 0.003223419189453125, 0.0033905506134033203, 0.0032231807708740234, 0.0031898021697998047, 0.0031747817993164062, 0.0032355785369873047, 0.003215789794921875, 0.0031321048736572266, 0.0032966136932373047, 0.003240823745727539, 0.003206491470336914, 0.0032029151916503906, 0.003438234329223633, 0.0034155845642089844, 0.0034036636352539062, 0.0032694339752197266, 0.0033447742462158203, 0.003184080123901367, 0.003267049789428711, 0.003156423568725586, 0.003205537796020508, 0.0032629966735839844, 0.003363370895385742, 0.003429889678955078, 0.003242969512939453, 0.003236055374145508, 0.0032045841217041016, 0.0031163692474365234, 0.00318145751953125, 0.003124237060546875, 0.003103494644165039, 0.003405332565307617, 0.0033638477325439453, 0.0030829906463623047, 0.0031478404998779297, 0.0033097267150878906, 0.0033986568450927734, 0.003200531005859375, 0.0032501220703125, 0.0032019615173339844, 0.003508329391479492, 0.003217935562133789, 0.0033941268920898438, 0.0032532215118408203, 0.0034856796264648438, 0.003347635269165039, 0.0032622814178466797, 0.0034036636352539062, 0.0033049583435058594, 0.003237485885620117, 0.0032737255096435547, 0.003231048583984375, 0.0032889842987060547, 0.003222942352294922, 0.003261566162109375, 0.0032629966735839844, 0.003255128860473633, 0.0032079219818115234, 0.003221750259399414, 0.0035161972045898438, 0.0032482147216796875, 0.0032584667205810547, 0.003375530242919922, 0.0035254955291748047, 0.0033037662506103516, 0.0034885406494140625, 0.0032761096954345703, 0.0034477710723876953, 0.003254413604736328, 0.0033278465270996094, 0.003452301025390625, 0.0032167434692382812, 0.003258943557739258, 0.003202676773071289, 0.0033102035522460938, 0.003454446792602539, 0.0034885406494140625, 0.003327608108520508, 0.0035262107849121094, 0.0031502246856689453, 0.003262758255004883, 0.0032167434692382812, 0.0034677982330322266, 0.003492116928100586, 0.0035660266876220703, 0.0034437179565429688, 0.003173828125, 0.003248929977416992, 0.003383636474609375, 0.0032587051391601562, 0.0032744407653808594, 0.003185749053955078, 0.0031948089599609375, 0.0033469200134277344, 0.003456592559814453, 0.003368377685546875, 0.0032510757446289062, 0.0032377243041992188, 0.0033681392669677734, 0.0032961368560791016, 0.0031943321228027344, 0.0033266544342041016, 0.0033702850341796875, 0.0032203197479248047, 0.003228425979614258, 0.0032782554626464844, 0.0032329559326171875, 0.0031850337982177734, 0.003429412841796875, 0.003441333770751953, 0.0031702518463134766, 0.0031690597534179688, 0.0033371448516845703, 0.003216266632080078, 0.0033516883850097656, 0.003212451934814453, 0.0032978057861328125, 0.0034754276275634766, 0.0034279823303222656, 0.0032134056091308594, 0.003420591354370117, 0.0032989978790283203, 0.003255605697631836, 0.003390073776245117, 0.003216981887817383, 0.00322723388671875, 0.003220796585083008, 0.0032100677490234375, 0.003297090530395508, 0.003251314163208008, 0.0032215118408203125, 0.0032134056091308594, 0.0033850669860839844, 0.0033538341522216797, 0.0032362937927246094, 0.0033524036407470703, 0.003465890884399414, 0.003473043441772461, 0.0032501220703125, 0.0031888484954833984, 0.0035054683685302734, 0.0032501220703125, 0.0032546520233154297, 0.003328084945678711, 0.0032825469970703125, 0.0032465457916259766, 0.0032303333282470703, 0.0032186508178710938, 0.0034227371215820312, 0.003258228302001953, 0.0032012462615966797, 0.0032224655151367188, 0.0031762123107910156, 0.0034890174865722656, 0.003215312957763672, 0.003247976303100586, 0.003239870071411133, 0.0033271312713623047, 0.003237009048461914, 0.0032830238342285156, 0.0032584667205810547, 0.003201007843017578, 0.003332853317260742, 0.0032529830932617188, 0.0032265186309814453, 0.003213644027709961, 0.0031838417053222656, 0.0035064220428466797, 0.0032684803009033203, 0.0034685134887695312, 0.0032498836517333984, 0.0033533573150634766, 0.003445148468017578, 0.0032274723052978516, 0.0032367706298828125, 0.003520965576171875, 0.0032629966735839844, 0.0033457279205322266, 0.003186941146850586, 0.0032393932342529297, 0.0032410621643066406, 0.003218412399291992, 0.003339052200317383, 0.00325775146484375, 0.0032095909118652344, 0.0032377243041992188, 0.003342866897583008, 0.0032434463500976562, 0.0032393932342529297, 0.0033190250396728516, 0.0032918453216552734, 0.003299236297607422, 0.003340482711791992, 0.003333568572998047, 0.003186941146850586, 0.0032806396484375, 0.003248929977416992, 0.0032126903533935547, 0.0032033920288085938, 0.0033445358276367188, 0.003210783004760742, 0.0031528472900390625, 0.003195047378540039, 0.0032529830932617188, 0.0030820369720458984, 0.0032470226287841797, 0.0033490657806396484, 0.003692150115966797, 0.003668546676635742, 0.0033321380615234375, 0.0033829212188720703, 0.003312349319458008, 0.0032427310943603516, 0.003294706344604492, 0.0032820701599121094, 0.0035924911499023438, 0.0033295154571533203, 0.0036232471466064453, 0.003304004669189453, 0.0033304691314697266, 0.003293752670288086, 0.0032949447631835938, 0.003273487091064453, 0.0033121109008789062, 0.0032486915588378906, 0.0035283565521240234, 0.0032205581665039062, 0.003512144088745117, 0.0033822059631347656, 0.0032317638397216797, 0.003297567367553711, 0.003376483917236328, 0.0035028457641601562, 0.0033502578735351562, 0.003305196762084961, 0.0032854080200195312, 0.0032873153686523438, 0.0032444000244140625, 0.003377199172973633, 0.0033135414123535156, 0.0033621788024902344, 0.003258228302001953, 0.003270864486694336, 0.0034928321838378906, 0.0033915042877197266, 0.0033495426177978516, 0.0034575462341308594, 0.0032761096954345703, 0.0034003257751464844, 0.003345489501953125, 0.0033655166625976562, 0.003197193145751953, 0.0033283233642578125, 0.0031986236572265625, 0.0033369064331054688, 0.003583669662475586, 0.0032956600189208984, 0.003335237503051758, 0.00327301025390625, 0.0032896995544433594, 0.0034012794494628906, 0.003487825393676758, 0.003267049789428711, 0.0032694339752197266, 0.003292083740234375, 0.003331422805786133, 0.0035212039947509766, 0.00333404541015625, 0.003360748291015625, 0.003317117691040039, 0.0032579898834228516, 0.003391742706298828, 0.003297090530395508, 0.0032358169555664062, 0.0032460689544677734, 0.003442525863647461, 0.003322601318359375, 0.0034551620483398438, 0.003362894058227539, 0.003542184829711914, 0.0033981800079345703, 0.003263711929321289, 0.003484487533569336, 0.0033235549926757812, 0.0034198760986328125, 0.0033111572265625, 0.003261089324951172, 0.003503561019897461, 0.0033092498779296875, 0.0034563541412353516, 0.0034182071685791016, 0.0034863948822021484, 0.0032987594604492188, 0.0032792091369628906, 0.00331878662109375, 0.0032987594604492188, 0.0032320022583007812, 0.0032470226287841797, 0.003246307373046875, 0.003238677978515625, 0.0033354759216308594, 0.0032935142517089844, 0.0032606124877929688, 0.0033686161041259766, 0.003571033477783203, 0.0032873153686523438, 0.0033121109008789062, 0.0035772323608398438, 0.0033011436462402344, 0.003254413604736328, 0.0032563209533691406, 0.0034220218658447266, 0.003313779830932617, 0.0033485889434814453, 0.0032532215118408203, 0.003285646438598633, 0.0032198429107666016, 0.003255128860473633, 0.003232240676879883, 0.0032987594604492188, 0.003291606903076172, 0.003337860107421875, 0.0032918453216552734, 0.003222227096557617, 0.003359079360961914, 0.0033049583435058594, 0.0033576488494873047, 0.0033016204833984375, 0.0033915042877197266, 0.003457307815551758, 0.003312826156616211, 0.0034513473510742188, 0.0035223960876464844, 0.0032699108123779297, 0.003453493118286133, 0.0033044815063476562, 0.0033185482025146484, 0.0033152103424072266, 0.0032966136932373047, 0.0034537315368652344, 0.0032820701599121094, 0.003457307815551758, 0.0033059120178222656, 0.003467559814453125, 0.0032885074615478516, 0.003407716751098633, 0.003315448760986328, 0.0033693313598632812, 0.003318309783935547, 0.003355741500854492, 0.0031969547271728516, 0.003320455551147461, 0.0032606124877929688, 0.0032711029052734375, 0.0033826828002929688, 0.0032644271850585938, 0.003322124481201172, 0.003306865692138672, 0.003243684768676758, 0.0034635066986083984, 0.003343820571899414, 0.003330707550048828, 0.0034894943237304688, 0.003244638442993164, 0.0033140182495117188, 0.0034356117248535156, 0.0031685829162597656, 0.0032002925872802734, 0.003195524215698242, 0.003160715103149414, 0.0030927658081054688, 0.0033326148986816406, 0.0034825801849365234, 0.0032625198364257812, 0.0032587051391601562, 0.0031859874725341797, 0.003333568572998047, 0.003451824188232422, 0.0033283233642578125, 0.0034487247467041016, 0.0033063888549804688, 0.0033240318298339844, 0.0032808780670166016, 0.003335237503051758, 0.003517627716064453, 0.00330352783203125, 0.003293752670288086, 0.0034852027893066406, 0.0034165382385253906, 0.0032241344451904297, 0.0032765865325927734, 0.0033533573150634766, 0.0033080577850341797, 0.003310680389404297, 0.003249645233154297, 0.003377199172973633, 0.0034928321838378906, 0.003228902816772461, 0.003269672393798828, 0.0033397674560546875, 0.003269195556640625, 0.00334930419921875, 0.0032837390899658203, 0.0033521652221679688, 0.0033278465270996094, 0.0033941268920898438, 0.003422975540161133, 0.0034437179565429688, 0.003317594528198242, 0.003268003463745117, 0.003246784210205078, 0.0034105777740478516, 0.0034329891204833984, 0.003275156021118164, 0.003319263458251953, 0.003285646438598633, 0.0034635066986083984, 0.003325939178466797, 0.003303050994873047, 0.003446340560913086, 0.0032889842987060547, 0.0034012794494628906, 0.003316164016723633, 0.0033981800079345703, 0.003275632858276367, 0.0033121109008789062, 0.003494739532470703, 0.0033059120178222656, 0.003412961959838867, 0.0033173561096191406, 0.003260374069213867, 0.0034456253051757812, 0.003337860107421875, 0.003282785415649414, 0.0032806396484375, 0.0035524368286132812, 0.0034379959106445312, 0.003255605697631836, 0.003540515899658203, 0.003407716751098633, 0.0035314559936523438, 0.0034787654876708984, 0.0033016204833984375, 0.0034275054931640625, 0.003296375274658203, 0.003513336181640625, 0.003469228744506836, 0.00334930419921875, 0.0035054683685302734, 0.003293752670288086, 0.003244161605834961, 0.0034618377685546875, 0.003289937973022461, 0.003264188766479492, 0.0035212039947509766, 0.0035054683685302734, 0.003329038619995117, 0.003491640090942383, 0.0034232139587402344, 0.0032503604888916016, 0.0033109188079833984, 0.0032684803009033203, 0.0035123825073242188, 0.003396749496459961, 0.003254413604736328, 0.003262758255004883, 0.00323486328125, 0.0035796165466308594, 0.003406524658203125, 0.0034987926483154297, 0.0031795501708984375, 0.003446817398071289, 0.003426074981689453, 0.0032744407653808594, 0.0034210681915283203, 0.0035424232482910156, 0.0033147335052490234, 0.0033235549926757812, 0.0033037662506103516, 0.003340482711791992, 0.0033168792724609375, 0.0033109188079833984, 0.003298044204711914, 0.0033583641052246094, 0.003321409225463867, 0.0034084320068359375, 0.003345012664794922, 0.003324747085571289, 0.0032618045806884766, 0.003331899642944336, 0.0032508373260498047, 0.0032377243041992188, 0.003274202346801758, 0.003247499465942383, 0.0034003257751464844, 0.0032601356506347656, 0.003335237503051758, 0.003361225128173828, 0.0033571720123291016, 0.0033926963806152344, 0.0032896995544433594, 0.0035355091094970703, 0.003451824188232422, 0.003278017044067383, 0.003267049789428711, 0.0033354759216308594, 0.003326416015625, 0.003207683563232422, 0.0032477378845214844, 0.0035033226013183594, 0.003465414047241211, 0.003252744674682617, 0.003659963607788086, 0.003396272659301758, 0.003480195999145508, 0.00333404541015625, 0.0033111572265625, 0.0032367706298828125, 0.0032396316528320312, 0.003268718719482422, 0.0033216476440429688, 0.0034313201904296875, 0.0032858848571777344, 0.003282785415649414, 0.003220796585083008, 0.003193378448486328, 0.003162384033203125, 0.0032966136932373047, 0.003295421600341797, 0.0032501220703125, 0.0031785964965820312, 0.0031588077545166016, 0.0031785964965820312, 0.0031337738037109375, 0.0031325817108154297, 0.003324270248413086, 0.0032477378845214844, 0.003136157989501953, 0.0031266212463378906, 0.003362417221069336, 0.003152608871459961, 0.003145933151245117, 0.003359556198120117, 0.003106832504272461, 0.003496408462524414, 0.0031592845916748047, 0.003134012222290039, 0.003145456314086914, 0.0032067298889160156, 0.003154277801513672, 0.003194093704223633, 0.0031821727752685547, 0.0031740665435791016, 0.0031309127807617188, 0.003276348114013672, 0.003385305404663086, 0.003153085708618164, 0.0032062530517578125, 0.003230571746826172, 0.003169536590576172, 0.0031690597534179688, 0.004109382629394531, 0.003241300582885742, 0.003757476806640625, 0.0033636093139648438, 0.0033943653106689453, 0.003191232681274414, 0.0032663345336914062, 0.0031578540802001953, 0.0032622814178466797, 0.003133058547973633, 0.0031418800354003906, 0.0032236576080322266, 0.0033295154571533203, 0.0031974315643310547, 0.003160238265991211, 0.003141164779663086, 0.003146648406982422, 0.0030968189239501953, 0.0032987594604492188, 0.0031719207763671875, 0.003428220748901367, 0.0032262802124023438, 0.003149747848510742, 0.003218412399291992, 0.0031652450561523438, 0.0033817291259765625, 0.0032875537872314453, 0.0034389495849609375, 0.003180265426635742, 0.0031919479370117188, 0.003171205520629883, 0.0031423568725585938, 0.0031299591064453125, 0.003187894821166992, 0.0031595230102539062, 0.0031337738037109375, 0.003131389617919922, 0.0032088756561279297, 0.003195047378540039, 0.0034961700439453125, 0.0030596256256103516, 0.0032324790954589844, 0.003168344497680664, 0.003128528594970703, 0.0034399032592773438, 0.0031747817993164062, 0.0031583309173583984, 0.003275632858276367, 0.0033419132232666016, 0.0031692981719970703, 0.0031633377075195312, 0.0031332969665527344, 0.0031347274780273438, 0.0035610198974609375, 0.0032172203063964844, 0.003118276596069336, 0.003121614456176758, 0.003126382827758789, 0.003106832504272461, 0.003207683563232422, 0.003262042999267578, 0.003347158432006836, 0.0031998157501220703, 0.003373861312866211, 0.0031404495239257812, 0.0031299591064453125, 0.003126859664916992, 0.0030541419982910156, 0.0030350685119628906, 0.0030019283294677734, 0.0029973983764648438, 0.0029888153076171875, 0.0030791759490966797, 0.0030481815338134766, 0.002996206283569336, 0.0030143260955810547, 0.003200054168701172, 0.0032079219818115234, 0.00302886962890625, 0.0030705928802490234, 0.003143310546875, 0.0030770301818847656, 0.003058195114135742, 0.0030012130737304688, 0.0032007694244384766, 0.0030364990234375, 0.0030243396759033203, 0.0032427310943603516, 0.0031545162200927734, 0.003101348876953125, 0.0030095577239990234, 0.0030655860900878906, 0.0031576156616210938, 0.0030641555786132812, 0.0030837059020996094, 0.0030159950256347656, 0.0030939579010009766, 0.003042936325073242, 0.0031087398529052734, 0.0034186840057373047, 0.003055572509765625, 0.0030546188354492188, 0.0030281543731689453, 0.0030040740966796875, 0.0031557083129882812, 0.0030345916748046875, 0.0030193328857421875, 0.002992391586303711, 0.0029859542846679688, 0.003009319305419922, 0.002993345260620117, 0.00299835205078125, 0.0030057430267333984, 0.0031175613403320312, 0.003112316131591797, 0.003069162368774414, 0.00322723388671875, 0.003018617630004883, 0.003058195114135742, 0.003069162368774414, 0.0030736923217773438, 0.0030319690704345703, 0.003008604049682617, 0.003006458282470703, 0.0029981136322021484, 0.0033707618713378906, 0.0032265186309814453, 0.0030508041381835938, 0.0030667781829833984, 0.0030164718627929688, 0.003331422805786133, 0.0030393600463867188, 0.003084421157836914, 0.003037691116333008, 0.003063678741455078, 0.0030095577239990234, 0.0031566619873046875, 0.0030586719512939453, 0.0030546188354492188, 0.0030279159545898438, 0.002983570098876953, 0.0030624866485595703, 0.003018617630004883, 0.003009796142578125, 0.0029997825622558594, 0.0030574798583984375, 0.0030448436737060547, 0.003014087677001953, 0.0030164718627929688, 0.0031404495239257812, 0.003282785415649414, 0.003159761428833008, 0.0030984878540039062, 0.003016948699951172, 0.00318145751953125, 0.0032320022583007812, 0.003233194351196289, 0.0031931400299072266, 0.0030002593994140625, 0.003005504608154297, 0.0033850669860839844, 0.0030553340911865234, 0.0032029151916503906, 0.003034353256225586, 0.0029931068420410156, 0.003228425979614258, 0.00302886962890625, 0.003044605255126953, 0.0030405521392822266, 0.0030188560485839844, 0.0030243396759033203, 0.0030112266540527344, 0.0030241012573242188, 0.003024578094482422, 0.0030117034912109375, 0.003269672393798828]\n"
     ]
    }
   ],
   "source": [
    "# Check timings\n",
    "print(timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Inference Time: 3.2824668884277344 seconds\n",
      "Number of Inferences: 1000\n",
      "Average Latency: 0.0032824668884277345 +- 0.00025755496218351115 seconds\n",
      "Average Throughput w/ Batch Size 1: 304.64892228630794 examples per second\n"
     ]
    }
   ],
   "source": [
    "# Benchmark TF\n",
    "timings = np.asarray(timings)\n",
    "delta_tf = np.sum(timings)\n",
    "average_latency_tf = np.mean(timings)\n",
    "std_latency_tf = np.std(timings)\n",
    "average_throughput_tf = batch_size * (1 / average_latency_tf)\n",
    "print('Total Inference Time: {} seconds'.format(delta_tf))\n",
    "print('Number of Inferences: {}'.format(len(timings)))\n",
    "print('Average Latency: {} +- {} seconds'.format(average_latency_tf, std_latency_tf))\n",
    "print('Average Throughput w/ Batch Size {}: {} examples per second'.format(batch_size, average_throughput_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running against TensorRT version 5.1.5\n"
     ]
    }
   ],
   "source": [
    "# Deserialize frozen graph\n",
    "with tf.gfile.GFile(model_path, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "# Create TF-TRT Graph\n",
    "trt_graph = trt.create_inference_graph(\n",
    "    input_graph_def=graph_def,\n",
    "    outputs=[output_node_name],\n",
    "    max_batch_size=batch_size,\n",
    "    is_dynamic_op=True,\n",
    "    max_workspace_size_bytes=2<<20,\n",
    "    precision_mode=precision_mode)\n",
    "\n",
    "# Serialize TF-TRT graph\n",
    "with tf.gfile.GFile(tftrt_graph_path, 'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "\n",
    "del trt_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing TF-TRT Graph...\n",
      "TF-TRT Graph imported!\n",
      "['tower_0/inference_input', 'tower_0/Const', 'tower_0/div', 'tower_0/Const_1', 'tower_0/Sub', 'tower_0/Const_2', 'tower_0/Mul', 'block_1/conv2d/kernel', 'tower_0/block_1/conv2d/Conv2D', 'block_4/conv2d/kernel', 'block_9/conv2d/kernel', 'tower_0/Reshape/shape', 'dense_1/kernel', 'dense_1/bias', 'dense_2/kernel', 'dense_2/bias', 'block_1/conv2d_1/kernel', 'tower_0/block_1/conv2d_2/Conv2D', 'tower_0/block_1/conv2d_2/Conv2D_bn_offset', 'tower_0/block_1/batch_normalization/FusedBatchNorm', 'tower_0/block_1/Relu', 'block_1/conv2d_2/kernel', 'tower_0/block_1/conv2d_3/Conv2D', 'tower_0/block_1/conv2d_3/Conv2D_bn_offset', 'tower_0/block_1/batch_normalization_2/FusedBatchNorm', 'tower_0/add', 'tower_0/Relu', 'tower_0/average_pooling2d/AvgPool', 'block_2/conv2d/kernel', 'tower_0/block_2/conv2d/Conv2D', 'tower_0/block_2/conv2d/Conv2D_bn_offset', 'tower_0/block_2/batch_normalization/FusedBatchNorm', 'tower_0/block_2/Relu', 'block_2/conv2d_1/kernel', 'tower_0/block_2/conv2d_2/Conv2D', 'tower_0/block_2/conv2d_2/Conv2D_bn_offset', 'tower_0/block_2/batch_normalization_2/FusedBatchNorm', 'tower_0/add_1', 'tower_0/Relu_1', 'block_3/conv2d/kernel', 'tower_0/block_3/conv2d/Conv2D', 'tower_0/block_3/conv2d/Conv2D_bn_offset', 'tower_0/block_3/batch_normalization/FusedBatchNorm', 'tower_0/block_3/Relu', 'block_3/conv2d_1/kernel', 'tower_0/block_3/conv2d_2/Conv2D', 'tower_0/block_3/conv2d_2/Conv2D_bn_offset', 'tower_0/block_3/batch_normalization_2/FusedBatchNorm', 'tower_0/add_2', 'tower_0/Relu_2', 'tower_0/block_4/conv2d/Conv2D', 'block_4/conv2d_1/kernel', 'tower_0/block_4/conv2d_2/Conv2D', 'tower_0/block_4/conv2d_2/Conv2D_bn_offset', 'tower_0/block_4/batch_normalization/FusedBatchNorm', 'tower_0/block_4/Relu', 'block_4/conv2d_2/kernel', 'tower_0/block_4/conv2d_3/Conv2D', 'tower_0/block_4/conv2d_3/Conv2D_bn_offset', 'tower_0/block_4/batch_normalization_2/FusedBatchNorm', 'tower_0/add_3', 'tower_0/Relu_3', 'block_5/conv2d/kernel', 'tower_0/block_5/conv2d/Conv2D', 'tower_0/block_5/conv2d/Conv2D_bn_offset', 'tower_0/block_5/batch_normalization/FusedBatchNorm', 'tower_0/block_5/Relu', 'block_5/conv2d_1/kernel', 'tower_0/block_5/conv2d_2/Conv2D', 'tower_0/block_5/conv2d_2/Conv2D_bn_offset', 'tower_0/block_5/batch_normalization_2/FusedBatchNorm', 'tower_0/add_4', 'tower_0/Relu_4', 'block_6/conv2d/kernel', 'tower_0/block_6/conv2d/Conv2D', 'tower_0/block_6/conv2d/Conv2D_bn_offset', 'tower_0/block_6/batch_normalization/FusedBatchNorm', 'tower_0/block_6/Relu', 'block_6/conv2d_1/kernel', 'tower_0/block_6/conv2d_2/Conv2D', 'tower_0/block_6/conv2d_2/Conv2D_bn_offset', 'tower_0/block_6/batch_normalization_2/FusedBatchNorm', 'tower_0/add_5', 'tower_0/Relu_5', 'block_7/conv2d/kernel', 'tower_0/block_7/conv2d/Conv2D', 'tower_0/block_7/conv2d/Conv2D_bn_offset', 'tower_0/block_7/batch_normalization/FusedBatchNorm', 'tower_0/block_7/Relu', 'block_7/conv2d_1/kernel', 'tower_0/block_7/conv2d_2/Conv2D', 'tower_0/block_7/conv2d_2/Conv2D_bn_offset', 'tower_0/block_7/batch_normalization_2/FusedBatchNorm', 'tower_0/add_6', 'tower_0/Relu_6', 'block_8/conv2d/kernel', 'tower_0/block_8/conv2d/Conv2D', 'tower_0/block_8/conv2d/Conv2D_bn_offset', 'tower_0/block_8/batch_normalization/FusedBatchNorm', 'tower_0/block_8/Relu', 'block_8/conv2d_1/kernel', 'tower_0/block_8/conv2d_2/Conv2D', 'tower_0/block_8/conv2d_2/Conv2D_bn_offset', 'tower_0/block_8/batch_normalization_2/FusedBatchNorm', 'tower_0/add_7', 'tower_0/Relu_7', 'tower_0/block_9/conv2d/Conv2D', 'block_9/conv2d_1/kernel', 'tower_0/block_9/conv2d_2/Conv2D', 'tower_0/block_9/conv2d_2/Conv2D_bn_offset', 'tower_0/block_9/batch_normalization/FusedBatchNorm', 'tower_0/block_9/Relu', 'block_9/conv2d_2/kernel', 'tower_0/block_9/conv2d_3/Conv2D', 'tower_0/block_9/conv2d_3/Conv2D_bn_offset', 'tower_0/block_9/batch_normalization_2/FusedBatchNorm', 'tower_0/add_8', 'tower_0/Relu_8', 'block_10/conv2d/kernel', 'tower_0/block_10/conv2d/Conv2D', 'tower_0/block_10/conv2d/Conv2D_bn_offset', 'tower_0/block_10/batch_normalization/FusedBatchNorm', 'tower_0/block_10/Relu', 'block_10/conv2d_1/kernel', 'tower_0/block_10/conv2d_2/Conv2D', 'tower_0/block_10/conv2d_2/Conv2D_bn_offset', 'tower_0/block_10/batch_normalization_2/FusedBatchNorm', 'tower_0/add_9', 'tower_0/Relu_9', 'block_11/conv2d/kernel', 'tower_0/block_11/conv2d/Conv2D', 'tower_0/block_11/conv2d/Conv2D_bn_offset', 'tower_0/block_11/batch_normalization/FusedBatchNorm', 'tower_0/block_11/Relu', 'block_11/conv2d_1/kernel', 'tower_0/block_11/conv2d_2/Conv2D', 'tower_0/block_11/conv2d_2/Conv2D_bn_offset', 'tower_0/block_11/batch_normalization_2/FusedBatchNorm', 'tower_0/add_10', 'tower_0/Relu_10', 'tower_0/average_pooling2d_2/AvgPool', 'tower_0/Reshape', 'tower_0/dense_1/MatMul', 'tower_0/dense_1/BiasAdd', 'tower_0/dense_1/Relu', 'tower_0/dense_2/MatMul', 'tower_0/dense_2/BiasAdd', 'tower_0/inference_output', 'import/tower_0/inference_input', 'import/tower_0/Const_1', 'import/tower_0/Const_2', 'import/block_1/conv2d/kernel', 'import/block_4/conv2d/kernel', 'import/block_9/conv2d/kernel', 'import/tower_0/Reshape/shape', 'import/block_1/conv2d_1/kernel', 'import/tower_0/block_1/conv2d_2/Conv2D_bn_offset', 'import/block_1/conv2d_2/kernel', 'import/tower_0/block_1/conv2d_3/Conv2D_bn_offset', 'import/block_2/conv2d/kernel', 'import/tower_0/block_2/conv2d/Conv2D_bn_offset', 'import/block_2/conv2d_1/kernel', 'import/tower_0/block_2/conv2d_2/Conv2D_bn_offset', 'import/block_3/conv2d/kernel', 'import/tower_0/block_3/conv2d/Conv2D_bn_offset', 'import/block_3/conv2d_1/kernel', 'import/tower_0/block_3/conv2d_2/Conv2D_bn_offset', 'import/block_4/conv2d_1/kernel', 'import/tower_0/block_4/conv2d_2/Conv2D_bn_offset', 'import/block_4/conv2d_2/kernel', 'import/tower_0/block_4/conv2d_3/Conv2D_bn_offset', 'import/block_5/conv2d/kernel', 'import/tower_0/block_5/conv2d/Conv2D_bn_offset', 'import/block_5/conv2d_1/kernel', 'import/tower_0/block_5/conv2d_2/Conv2D_bn_offset', 'import/block_6/conv2d/kernel', 'import/tower_0/block_6/conv2d/Conv2D_bn_offset', 'import/block_6/conv2d_1/kernel', 'import/tower_0/block_6/conv2d_2/Conv2D_bn_offset', 'import/block_7/conv2d/kernel', 'import/tower_0/block_7/conv2d/Conv2D_bn_offset', 'import/block_7/conv2d_1/kernel', 'import/tower_0/block_7/conv2d_2/Conv2D_bn_offset', 'import/block_8/conv2d/kernel', 'import/tower_0/block_8/conv2d/Conv2D_bn_offset', 'import/block_8/conv2d_1/kernel', 'import/tower_0/block_8/conv2d_2/Conv2D_bn_offset', 'import/block_9/conv2d_1/kernel', 'import/tower_0/block_9/conv2d_2/Conv2D_bn_offset', 'import/block_9/conv2d_2/kernel', 'import/tower_0/block_9/conv2d_3/Conv2D_bn_offset', 'import/block_10/conv2d/kernel', 'import/tower_0/block_10/conv2d/Conv2D_bn_offset', 'import/block_10/conv2d_1/kernel', 'import/tower_0/block_10/conv2d_2/Conv2D_bn_offset', 'import/block_11/conv2d/kernel', 'import/tower_0/block_11/conv2d/Conv2D_bn_offset', 'import/block_11/conv2d_1/kernel', 'import/tower_0/block_11/conv2d_2/Conv2D_bn_offset', 'import/ConstantFolding/tower_0/div_recip', 'import/PermConstNHWCToNCHW-LayoutOptimizer', 'import/PermConstNCHWToNHWC-LayoutOptimizer', 'import/tower_0/div', 'import/tower_0/Sub', 'import/tower_0/Mul', 'import/tower_0/block_1/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer', 'import/tower_0/block_1/conv2d_2/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer', 'import/tower_0/block_1/conv2d/Conv2D', 'import/tower_0/block_1/conv2d_2/Conv2D', 'import/tower_0/block_1/batch_normalization/FusedBatchNorm', 'import/tower_0/block_1/Relu', 'import/tower_0/block_1/conv2d_3/Conv2D', 'import/tower_0/block_1/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add', 'import/tower_0/Relu', 'import/tower_0/average_pooling2d/AvgPool', 'import/tower_0/block_2/conv2d/Conv2D', 'import/tower_0/block_2/batch_normalization/FusedBatchNorm', 'import/tower_0/block_2/Relu', 'import/tower_0/block_2/conv2d_2/Conv2D', 'import/tower_0/block_2/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_1', 'import/tower_0/Relu_1', 'import/tower_0/block_3/conv2d/Conv2D', 'import/tower_0/block_3/batch_normalization/FusedBatchNorm', 'import/tower_0/block_3/Relu', 'import/tower_0/block_3/conv2d_2/Conv2D', 'import/tower_0/block_3/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_2', 'import/tower_0/Relu_2', 'import/tower_0/block_4/conv2d/Conv2D', 'import/tower_0/block_4/conv2d_2/Conv2D', 'import/tower_0/block_4/batch_normalization/FusedBatchNorm', 'import/tower_0/block_4/Relu', 'import/tower_0/block_4/conv2d_3/Conv2D', 'import/tower_0/block_4/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_3', 'import/tower_0/Relu_3', 'import/tower_0/block_5/conv2d/Conv2D', 'import/tower_0/block_5/batch_normalization/FusedBatchNorm', 'import/tower_0/block_5/Relu', 'import/tower_0/block_5/conv2d_2/Conv2D', 'import/tower_0/block_5/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_4', 'import/tower_0/Relu_4', 'import/tower_0/block_6/conv2d/Conv2D', 'import/tower_0/block_6/batch_normalization/FusedBatchNorm', 'import/tower_0/block_6/Relu', 'import/tower_0/block_6/conv2d_2/Conv2D', 'import/tower_0/block_6/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_5', 'import/tower_0/Relu_5', 'import/tower_0/block_7/conv2d/Conv2D', 'import/tower_0/block_7/batch_normalization/FusedBatchNorm', 'import/tower_0/block_7/Relu', 'import/tower_0/block_7/conv2d_2/Conv2D', 'import/tower_0/block_7/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_6', 'import/tower_0/Relu_6', 'import/tower_0/block_8/conv2d/Conv2D', 'import/tower_0/block_8/batch_normalization/FusedBatchNorm', 'import/tower_0/block_8/Relu', 'import/tower_0/block_8/conv2d_2/Conv2D', 'import/tower_0/block_8/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_7', 'import/tower_0/Relu_7', 'import/tower_0/block_9/conv2d/Conv2D', 'import/tower_0/block_9/conv2d_2/Conv2D', 'import/tower_0/block_9/batch_normalization/FusedBatchNorm', 'import/tower_0/block_9/Relu', 'import/tower_0/block_9/conv2d_3/Conv2D', 'import/tower_0/block_9/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_8', 'import/tower_0/Relu_8', 'import/tower_0/block_10/conv2d/Conv2D', 'import/tower_0/block_10/batch_normalization/FusedBatchNorm', 'import/tower_0/block_10/Relu', 'import/tower_0/block_10/conv2d_2/Conv2D', 'import/tower_0/block_10/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_9', 'import/tower_0/Relu_9', 'import/tower_0/block_11/conv2d/Conv2D', 'import/tower_0/block_11/batch_normalization/FusedBatchNorm', 'import/tower_0/block_11/Relu', 'import/tower_0/block_11/conv2d_2/Conv2D', 'import/tower_0/block_11/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_10', 'import/tower_0/Relu_10', 'import/tower_0/average_pooling2d_2/AvgPool', 'import/tower_0/average_pooling2d_2/AvgPool-0-0-TransposeNCHWToNHWC-LayoutOptimizer', 'import/tower_0/Reshape', 'import/TRTEngineOp_1', 'import/tower_0/inference_output']\n",
      "Running calibration data through TF-TRT Graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration data ran through TF-TRT Graph!\n",
      "Converting calibration graph to inference graph...\n",
      "Calibration graph converted to inference graph!\n"
     ]
    }
   ],
   "source": [
    "# calibrate the graph to INT8\n",
    "with tf.Session() as sess:\n",
    "    # Deserialize TF-TRT graph\n",
    "    with tf.gfile.GFile(tftrt_graph_path, 'rb') as f:\n",
    "        calib_graph = tf.GraphDef()\n",
    "        calib_graph.ParseFromString(f.read())\n",
    "    \n",
    "    # import TF-TRT graph\n",
    "    print('Importing TF-TRT Graph...')\n",
    "    output_node = tf.import_graph_def(calib_graph, return_elements=[output_node_name])\n",
    "    print('TF-TRT Graph imported!')\n",
    "    \n",
    "    tensor_names = [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    print(tensor_names)\n",
    "    \n",
    "    # modify node name\n",
    "    modified_input_node_name = 'import/{}'.format(input_node_name)\n",
    "    \n",
    "    print('Running calibration data through TF-TRT Graph...')\n",
    "    for i in range(n_calibration_loops):\n",
    "        # create random data for calibration - TODO replace this with calibration data\n",
    "        calibration_data = np.random.randint(low=0, high=256, size=(batch_size, n_height, n_width, n_channels))\n",
    "        calibration_data = calibration_data.astype(np.float32)\n",
    "        \n",
    "        # run net to dummy calibrate\n",
    "        _ = sess.run(output_node, feed_dict={modified_input_node_name: calibration_data})\n",
    "    print('Calibration data ran through TF-TRT Graph!')\n",
    "    \n",
    "    # calibrate graph\n",
    "    print('Converting calibration graph to inference graph...')\n",
    "    #calibrated_graph = trt.calib_graph_to_infer_graph(calib_graph, is_dynamic_op=True)\n",
    "    calibrated_graph = trt.calib_graph_to_infer_graph(calib_graph)\n",
    "    print('Calibration graph converted to inference graph!')\n",
    "    \n",
    "    # Serialize INT8 calibrated TF-TRT graph\n",
    "    with tf.gfile.GFile(tftrt_int8_graph_path, 'wb') as f:\n",
    "        f.write(calibrated_graph.SerializeToString())\n",
    "    \n",
    "    del calib_graph\n",
    "    del calibrated_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'import_1/tower_0/inference_output:0' shape=<unknown> dtype=float32>]\n",
      "['tower_0/inference_input', 'tower_0/Const', 'tower_0/div', 'tower_0/Const_1', 'tower_0/Sub', 'tower_0/Const_2', 'tower_0/Mul', 'block_1/conv2d/kernel', 'tower_0/block_1/conv2d/Conv2D', 'block_4/conv2d/kernel', 'block_9/conv2d/kernel', 'tower_0/Reshape/shape', 'dense_1/kernel', 'dense_1/bias', 'dense_2/kernel', 'dense_2/bias', 'block_1/conv2d_1/kernel', 'tower_0/block_1/conv2d_2/Conv2D', 'tower_0/block_1/conv2d_2/Conv2D_bn_offset', 'tower_0/block_1/batch_normalization/FusedBatchNorm', 'tower_0/block_1/Relu', 'block_1/conv2d_2/kernel', 'tower_0/block_1/conv2d_3/Conv2D', 'tower_0/block_1/conv2d_3/Conv2D_bn_offset', 'tower_0/block_1/batch_normalization_2/FusedBatchNorm', 'tower_0/add', 'tower_0/Relu', 'tower_0/average_pooling2d/AvgPool', 'block_2/conv2d/kernel', 'tower_0/block_2/conv2d/Conv2D', 'tower_0/block_2/conv2d/Conv2D_bn_offset', 'tower_0/block_2/batch_normalization/FusedBatchNorm', 'tower_0/block_2/Relu', 'block_2/conv2d_1/kernel', 'tower_0/block_2/conv2d_2/Conv2D', 'tower_0/block_2/conv2d_2/Conv2D_bn_offset', 'tower_0/block_2/batch_normalization_2/FusedBatchNorm', 'tower_0/add_1', 'tower_0/Relu_1', 'block_3/conv2d/kernel', 'tower_0/block_3/conv2d/Conv2D', 'tower_0/block_3/conv2d/Conv2D_bn_offset', 'tower_0/block_3/batch_normalization/FusedBatchNorm', 'tower_0/block_3/Relu', 'block_3/conv2d_1/kernel', 'tower_0/block_3/conv2d_2/Conv2D', 'tower_0/block_3/conv2d_2/Conv2D_bn_offset', 'tower_0/block_3/batch_normalization_2/FusedBatchNorm', 'tower_0/add_2', 'tower_0/Relu_2', 'tower_0/block_4/conv2d/Conv2D', 'block_4/conv2d_1/kernel', 'tower_0/block_4/conv2d_2/Conv2D', 'tower_0/block_4/conv2d_2/Conv2D_bn_offset', 'tower_0/block_4/batch_normalization/FusedBatchNorm', 'tower_0/block_4/Relu', 'block_4/conv2d_2/kernel', 'tower_0/block_4/conv2d_3/Conv2D', 'tower_0/block_4/conv2d_3/Conv2D_bn_offset', 'tower_0/block_4/batch_normalization_2/FusedBatchNorm', 'tower_0/add_3', 'tower_0/Relu_3', 'block_5/conv2d/kernel', 'tower_0/block_5/conv2d/Conv2D', 'tower_0/block_5/conv2d/Conv2D_bn_offset', 'tower_0/block_5/batch_normalization/FusedBatchNorm', 'tower_0/block_5/Relu', 'block_5/conv2d_1/kernel', 'tower_0/block_5/conv2d_2/Conv2D', 'tower_0/block_5/conv2d_2/Conv2D_bn_offset', 'tower_0/block_5/batch_normalization_2/FusedBatchNorm', 'tower_0/add_4', 'tower_0/Relu_4', 'block_6/conv2d/kernel', 'tower_0/block_6/conv2d/Conv2D', 'tower_0/block_6/conv2d/Conv2D_bn_offset', 'tower_0/block_6/batch_normalization/FusedBatchNorm', 'tower_0/block_6/Relu', 'block_6/conv2d_1/kernel', 'tower_0/block_6/conv2d_2/Conv2D', 'tower_0/block_6/conv2d_2/Conv2D_bn_offset', 'tower_0/block_6/batch_normalization_2/FusedBatchNorm', 'tower_0/add_5', 'tower_0/Relu_5', 'block_7/conv2d/kernel', 'tower_0/block_7/conv2d/Conv2D', 'tower_0/block_7/conv2d/Conv2D_bn_offset', 'tower_0/block_7/batch_normalization/FusedBatchNorm', 'tower_0/block_7/Relu', 'block_7/conv2d_1/kernel', 'tower_0/block_7/conv2d_2/Conv2D', 'tower_0/block_7/conv2d_2/Conv2D_bn_offset', 'tower_0/block_7/batch_normalization_2/FusedBatchNorm', 'tower_0/add_6', 'tower_0/Relu_6', 'block_8/conv2d/kernel', 'tower_0/block_8/conv2d/Conv2D', 'tower_0/block_8/conv2d/Conv2D_bn_offset', 'tower_0/block_8/batch_normalization/FusedBatchNorm', 'tower_0/block_8/Relu', 'block_8/conv2d_1/kernel', 'tower_0/block_8/conv2d_2/Conv2D', 'tower_0/block_8/conv2d_2/Conv2D_bn_offset', 'tower_0/block_8/batch_normalization_2/FusedBatchNorm', 'tower_0/add_7', 'tower_0/Relu_7', 'tower_0/block_9/conv2d/Conv2D', 'block_9/conv2d_1/kernel', 'tower_0/block_9/conv2d_2/Conv2D', 'tower_0/block_9/conv2d_2/Conv2D_bn_offset', 'tower_0/block_9/batch_normalization/FusedBatchNorm', 'tower_0/block_9/Relu', 'block_9/conv2d_2/kernel', 'tower_0/block_9/conv2d_3/Conv2D', 'tower_0/block_9/conv2d_3/Conv2D_bn_offset', 'tower_0/block_9/batch_normalization_2/FusedBatchNorm', 'tower_0/add_8', 'tower_0/Relu_8', 'block_10/conv2d/kernel', 'tower_0/block_10/conv2d/Conv2D', 'tower_0/block_10/conv2d/Conv2D_bn_offset', 'tower_0/block_10/batch_normalization/FusedBatchNorm', 'tower_0/block_10/Relu', 'block_10/conv2d_1/kernel', 'tower_0/block_10/conv2d_2/Conv2D', 'tower_0/block_10/conv2d_2/Conv2D_bn_offset', 'tower_0/block_10/batch_normalization_2/FusedBatchNorm', 'tower_0/add_9', 'tower_0/Relu_9', 'block_11/conv2d/kernel', 'tower_0/block_11/conv2d/Conv2D', 'tower_0/block_11/conv2d/Conv2D_bn_offset', 'tower_0/block_11/batch_normalization/FusedBatchNorm', 'tower_0/block_11/Relu', 'block_11/conv2d_1/kernel', 'tower_0/block_11/conv2d_2/Conv2D', 'tower_0/block_11/conv2d_2/Conv2D_bn_offset', 'tower_0/block_11/batch_normalization_2/FusedBatchNorm', 'tower_0/add_10', 'tower_0/Relu_10', 'tower_0/average_pooling2d_2/AvgPool', 'tower_0/Reshape', 'tower_0/dense_1/MatMul', 'tower_0/dense_1/BiasAdd', 'tower_0/dense_1/Relu', 'tower_0/dense_2/MatMul', 'tower_0/dense_2/BiasAdd', 'tower_0/inference_output', 'import/tower_0/inference_input', 'import/tower_0/Const_1', 'import/tower_0/Const_2', 'import/block_1/conv2d/kernel', 'import/block_4/conv2d/kernel', 'import/block_9/conv2d/kernel', 'import/tower_0/Reshape/shape', 'import/block_1/conv2d_1/kernel', 'import/tower_0/block_1/conv2d_2/Conv2D_bn_offset', 'import/block_1/conv2d_2/kernel', 'import/tower_0/block_1/conv2d_3/Conv2D_bn_offset', 'import/block_2/conv2d/kernel', 'import/tower_0/block_2/conv2d/Conv2D_bn_offset', 'import/block_2/conv2d_1/kernel', 'import/tower_0/block_2/conv2d_2/Conv2D_bn_offset', 'import/block_3/conv2d/kernel', 'import/tower_0/block_3/conv2d/Conv2D_bn_offset', 'import/block_3/conv2d_1/kernel', 'import/tower_0/block_3/conv2d_2/Conv2D_bn_offset', 'import/block_4/conv2d_1/kernel', 'import/tower_0/block_4/conv2d_2/Conv2D_bn_offset', 'import/block_4/conv2d_2/kernel', 'import/tower_0/block_4/conv2d_3/Conv2D_bn_offset', 'import/block_5/conv2d/kernel', 'import/tower_0/block_5/conv2d/Conv2D_bn_offset', 'import/block_5/conv2d_1/kernel', 'import/tower_0/block_5/conv2d_2/Conv2D_bn_offset', 'import/block_6/conv2d/kernel', 'import/tower_0/block_6/conv2d/Conv2D_bn_offset', 'import/block_6/conv2d_1/kernel', 'import/tower_0/block_6/conv2d_2/Conv2D_bn_offset', 'import/block_7/conv2d/kernel', 'import/tower_0/block_7/conv2d/Conv2D_bn_offset', 'import/block_7/conv2d_1/kernel', 'import/tower_0/block_7/conv2d_2/Conv2D_bn_offset', 'import/block_8/conv2d/kernel', 'import/tower_0/block_8/conv2d/Conv2D_bn_offset', 'import/block_8/conv2d_1/kernel', 'import/tower_0/block_8/conv2d_2/Conv2D_bn_offset', 'import/block_9/conv2d_1/kernel', 'import/tower_0/block_9/conv2d_2/Conv2D_bn_offset', 'import/block_9/conv2d_2/kernel', 'import/tower_0/block_9/conv2d_3/Conv2D_bn_offset', 'import/block_10/conv2d/kernel', 'import/tower_0/block_10/conv2d/Conv2D_bn_offset', 'import/block_10/conv2d_1/kernel', 'import/tower_0/block_10/conv2d_2/Conv2D_bn_offset', 'import/block_11/conv2d/kernel', 'import/tower_0/block_11/conv2d/Conv2D_bn_offset', 'import/block_11/conv2d_1/kernel', 'import/tower_0/block_11/conv2d_2/Conv2D_bn_offset', 'import/ConstantFolding/tower_0/div_recip', 'import/PermConstNHWCToNCHW-LayoutOptimizer', 'import/PermConstNCHWToNHWC-LayoutOptimizer', 'import/tower_0/div', 'import/tower_0/Sub', 'import/tower_0/Mul', 'import/tower_0/block_1/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer', 'import/tower_0/block_1/conv2d_2/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer', 'import/tower_0/block_1/conv2d/Conv2D', 'import/tower_0/block_1/conv2d_2/Conv2D', 'import/tower_0/block_1/batch_normalization/FusedBatchNorm', 'import/tower_0/block_1/Relu', 'import/tower_0/block_1/conv2d_3/Conv2D', 'import/tower_0/block_1/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add', 'import/tower_0/Relu', 'import/tower_0/average_pooling2d/AvgPool', 'import/tower_0/block_2/conv2d/Conv2D', 'import/tower_0/block_2/batch_normalization/FusedBatchNorm', 'import/tower_0/block_2/Relu', 'import/tower_0/block_2/conv2d_2/Conv2D', 'import/tower_0/block_2/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_1', 'import/tower_0/Relu_1', 'import/tower_0/block_3/conv2d/Conv2D', 'import/tower_0/block_3/batch_normalization/FusedBatchNorm', 'import/tower_0/block_3/Relu', 'import/tower_0/block_3/conv2d_2/Conv2D', 'import/tower_0/block_3/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_2', 'import/tower_0/Relu_2', 'import/tower_0/block_4/conv2d/Conv2D', 'import/tower_0/block_4/conv2d_2/Conv2D', 'import/tower_0/block_4/batch_normalization/FusedBatchNorm', 'import/tower_0/block_4/Relu', 'import/tower_0/block_4/conv2d_3/Conv2D', 'import/tower_0/block_4/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_3', 'import/tower_0/Relu_3', 'import/tower_0/block_5/conv2d/Conv2D', 'import/tower_0/block_5/batch_normalization/FusedBatchNorm', 'import/tower_0/block_5/Relu', 'import/tower_0/block_5/conv2d_2/Conv2D', 'import/tower_0/block_5/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_4', 'import/tower_0/Relu_4', 'import/tower_0/block_6/conv2d/Conv2D', 'import/tower_0/block_6/batch_normalization/FusedBatchNorm', 'import/tower_0/block_6/Relu', 'import/tower_0/block_6/conv2d_2/Conv2D', 'import/tower_0/block_6/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_5', 'import/tower_0/Relu_5', 'import/tower_0/block_7/conv2d/Conv2D', 'import/tower_0/block_7/batch_normalization/FusedBatchNorm', 'import/tower_0/block_7/Relu', 'import/tower_0/block_7/conv2d_2/Conv2D', 'import/tower_0/block_7/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_6', 'import/tower_0/Relu_6', 'import/tower_0/block_8/conv2d/Conv2D', 'import/tower_0/block_8/batch_normalization/FusedBatchNorm', 'import/tower_0/block_8/Relu', 'import/tower_0/block_8/conv2d_2/Conv2D', 'import/tower_0/block_8/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_7', 'import/tower_0/Relu_7', 'import/tower_0/block_9/conv2d/Conv2D', 'import/tower_0/block_9/conv2d_2/Conv2D', 'import/tower_0/block_9/batch_normalization/FusedBatchNorm', 'import/tower_0/block_9/Relu', 'import/tower_0/block_9/conv2d_3/Conv2D', 'import/tower_0/block_9/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_8', 'import/tower_0/Relu_8', 'import/tower_0/block_10/conv2d/Conv2D', 'import/tower_0/block_10/batch_normalization/FusedBatchNorm', 'import/tower_0/block_10/Relu', 'import/tower_0/block_10/conv2d_2/Conv2D', 'import/tower_0/block_10/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_9', 'import/tower_0/Relu_9', 'import/tower_0/block_11/conv2d/Conv2D', 'import/tower_0/block_11/batch_normalization/FusedBatchNorm', 'import/tower_0/block_11/Relu', 'import/tower_0/block_11/conv2d_2/Conv2D', 'import/tower_0/block_11/batch_normalization_2/FusedBatchNorm', 'import/tower_0/add_10', 'import/tower_0/Relu_10', 'import/tower_0/average_pooling2d_2/AvgPool', 'import/tower_0/average_pooling2d_2/AvgPool-0-0-TransposeNCHWToNHWC-LayoutOptimizer', 'import/tower_0/Reshape', 'import/TRTEngineOp_1', 'import/tower_0/inference_output', 'import_1/tower_0/inference_input', 'import_1/tower_0/Const_1', 'import_1/tower_0/Const_2', 'import_1/block_1/conv2d/kernel', 'import_1/block_4/conv2d/kernel', 'import_1/block_9/conv2d/kernel', 'import_1/tower_0/Reshape/shape', 'import_1/block_1/conv2d_1/kernel', 'import_1/tower_0/block_1/conv2d_2/Conv2D_bn_offset', 'import_1/block_1/conv2d_2/kernel', 'import_1/tower_0/block_1/conv2d_3/Conv2D_bn_offset', 'import_1/block_2/conv2d/kernel', 'import_1/tower_0/block_2/conv2d/Conv2D_bn_offset', 'import_1/block_2/conv2d_1/kernel', 'import_1/tower_0/block_2/conv2d_2/Conv2D_bn_offset', 'import_1/block_3/conv2d/kernel', 'import_1/tower_0/block_3/conv2d/Conv2D_bn_offset', 'import_1/block_3/conv2d_1/kernel', 'import_1/tower_0/block_3/conv2d_2/Conv2D_bn_offset', 'import_1/block_4/conv2d_1/kernel', 'import_1/tower_0/block_4/conv2d_2/Conv2D_bn_offset', 'import_1/block_4/conv2d_2/kernel', 'import_1/tower_0/block_4/conv2d_3/Conv2D_bn_offset', 'import_1/block_5/conv2d/kernel', 'import_1/tower_0/block_5/conv2d/Conv2D_bn_offset', 'import_1/block_5/conv2d_1/kernel', 'import_1/tower_0/block_5/conv2d_2/Conv2D_bn_offset', 'import_1/block_6/conv2d/kernel', 'import_1/tower_0/block_6/conv2d/Conv2D_bn_offset', 'import_1/block_6/conv2d_1/kernel', 'import_1/tower_0/block_6/conv2d_2/Conv2D_bn_offset', 'import_1/block_7/conv2d/kernel', 'import_1/tower_0/block_7/conv2d/Conv2D_bn_offset', 'import_1/block_7/conv2d_1/kernel', 'import_1/tower_0/block_7/conv2d_2/Conv2D_bn_offset', 'import_1/block_8/conv2d/kernel', 'import_1/tower_0/block_8/conv2d/Conv2D_bn_offset', 'import_1/block_8/conv2d_1/kernel', 'import_1/tower_0/block_8/conv2d_2/Conv2D_bn_offset', 'import_1/block_9/conv2d_1/kernel', 'import_1/tower_0/block_9/conv2d_2/Conv2D_bn_offset', 'import_1/block_9/conv2d_2/kernel', 'import_1/tower_0/block_9/conv2d_3/Conv2D_bn_offset', 'import_1/block_10/conv2d/kernel', 'import_1/tower_0/block_10/conv2d/Conv2D_bn_offset', 'import_1/block_10/conv2d_1/kernel', 'import_1/tower_0/block_10/conv2d_2/Conv2D_bn_offset', 'import_1/block_11/conv2d/kernel', 'import_1/tower_0/block_11/conv2d/Conv2D_bn_offset', 'import_1/block_11/conv2d_1/kernel', 'import_1/tower_0/block_11/conv2d_2/Conv2D_bn_offset', 'import_1/ConstantFolding/tower_0/div_recip', 'import_1/PermConstNHWCToNCHW-LayoutOptimizer', 'import_1/PermConstNCHWToNHWC-LayoutOptimizer', 'import_1/tower_0/div', 'import_1/tower_0/Sub', 'import_1/tower_0/Mul', 'import_1/tower_0/block_1/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer', 'import_1/tower_0/block_1/conv2d_2/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer', 'import_1/tower_0/block_1/conv2d/Conv2D', 'import_1/tower_0/block_1/conv2d_2/Conv2D', 'import_1/tower_0/block_1/batch_normalization/FusedBatchNorm', 'import_1/tower_0/block_1/Relu', 'import_1/tower_0/block_1/conv2d_3/Conv2D', 'import_1/tower_0/block_1/batch_normalization_2/FusedBatchNorm', 'import_1/tower_0/add', 'import_1/tower_0/Relu', 'import_1/tower_0/average_pooling2d/AvgPool', 'import_1/tower_0/block_2/conv2d/Conv2D', 'import_1/tower_0/block_2/batch_normalization/FusedBatchNorm', 'import_1/tower_0/block_2/Relu', 'import_1/tower_0/block_2/conv2d_2/Conv2D', 'import_1/tower_0/block_2/batch_normalization_2/FusedBatchNorm', 'import_1/tower_0/add_1', 'import_1/tower_0/Relu_1', 'import_1/tower_0/block_3/conv2d/Conv2D', 'import_1/tower_0/block_3/batch_normalization/FusedBatchNorm', 'import_1/tower_0/block_3/Relu', 'import_1/tower_0/block_3/conv2d_2/Conv2D', 'import_1/tower_0/block_3/batch_normalization_2/FusedBatchNorm', 'import_1/tower_0/add_2', 'import_1/tower_0/Relu_2', 'import_1/tower_0/block_4/conv2d/Conv2D', 'import_1/tower_0/block_4/conv2d_2/Conv2D', 'import_1/tower_0/block_4/batch_normalization/FusedBatchNorm', 'import_1/tower_0/block_4/Relu', 'import_1/tower_0/block_4/conv2d_3/Conv2D', 'import_1/tower_0/block_4/batch_normalization_2/FusedBatchNorm', 'import_1/tower_0/add_3', 'import_1/tower_0/Relu_3', 'import_1/tower_0/block_5/conv2d/Conv2D', 'import_1/tower_0/block_5/batch_normalization/FusedBatchNorm', 'import_1/tower_0/block_5/Relu', 'import_1/tower_0/block_5/conv2d_2/Conv2D', 'import_1/tower_0/block_5/batch_normalization_2/FusedBatchNorm', 'import_1/tower_0/add_4', 'import_1/tower_0/Relu_4', 'import_1/tower_0/block_6/conv2d/Conv2D', 'import_1/tower_0/block_6/batch_normalization/FusedBatchNorm', 'import_1/tower_0/block_6/Relu', 'import_1/tower_0/block_6/conv2d_2/Conv2D', 'import_1/tower_0/block_6/batch_normalization_2/FusedBatchNorm', 'import_1/tower_0/add_5', 'import_1/tower_0/Relu_5', 'import_1/tower_0/block_7/conv2d/Conv2D', 'import_1/tower_0/block_7/batch_normalization/FusedBatchNorm', 'import_1/tower_0/block_7/Relu', 'import_1/tower_0/block_7/conv2d_2/Conv2D', 'import_1/tower_0/block_7/batch_normalization_2/FusedBatchNorm', 'import_1/tower_0/add_6', 'import_1/tower_0/Relu_6', 'import_1/tower_0/block_8/conv2d/Conv2D', 'import_1/tower_0/block_8/batch_normalization/FusedBatchNorm', 'import_1/tower_0/block_8/Relu', 'import_1/tower_0/block_8/conv2d_2/Conv2D', 'import_1/tower_0/block_8/batch_normalization_2/FusedBatchNorm', 'import_1/tower_0/add_7', 'import_1/tower_0/Relu_7', 'import_1/tower_0/block_9/conv2d/Conv2D', 'import_1/tower_0/block_9/conv2d_2/Conv2D', 'import_1/tower_0/block_9/batch_normalization/FusedBatchNorm', 'import_1/tower_0/block_9/Relu', 'import_1/tower_0/block_9/conv2d_3/Conv2D', 'import_1/tower_0/block_9/batch_normalization_2/FusedBatchNorm', 'import_1/tower_0/add_8', 'import_1/tower_0/Relu_8', 'import_1/tower_0/block_10/conv2d/Conv2D', 'import_1/tower_0/block_10/batch_normalization/FusedBatchNorm', 'import_1/tower_0/block_10/Relu', 'import_1/tower_0/block_10/conv2d_2/Conv2D', 'import_1/tower_0/block_10/batch_normalization_2/FusedBatchNorm', 'import_1/tower_0/add_9', 'import_1/tower_0/Relu_9', 'import_1/tower_0/block_11/conv2d/Conv2D', 'import_1/tower_0/block_11/batch_normalization/FusedBatchNorm', 'import_1/tower_0/block_11/Relu', 'import_1/tower_0/block_11/conv2d_2/Conv2D', 'import_1/tower_0/block_11/batch_normalization_2/FusedBatchNorm', 'import_1/tower_0/add_10', 'import_1/tower_0/Relu_10', 'import_1/tower_0/average_pooling2d_2/AvgPool', 'import_1/tower_0/average_pooling2d_2/AvgPool-0-0-TransposeNCHWToNHWC-LayoutOptimizer', 'import_1/tower_0/Reshape', 'import_1/TRTEngineOp_1', 'import_1/tower_0/inference_output']\n"
     ]
    }
   ],
   "source": [
    "# calibrate the graph to INT8\n",
    "with tf.Session() as sess:\n",
    "    # Deserialize INT8 calibrated TF-TRT graph\n",
    "    with tf.gfile.GFile(tftrt_int8_graph_path, 'rb') as f:\n",
    "        tftrt_int8_graph = tf.GraphDef()\n",
    "        tftrt_int8_graph.ParseFromString(f.read())\n",
    "    \n",
    "    # modify node names\n",
    "    modified_input_node_name = 'import_1/{}'.format(input_node_name)\n",
    "    #modified_output_node_name = 'import/{}'.format(output_node_name)\n",
    "        \n",
    "    # Import the calibrated TF-TRT graph\n",
    "    output_node = tf.import_graph_def(tftrt_int8_graph, return_elements=[output_node_name])\n",
    "    #output_node = tf.import_graph_def(tftrt_int8_graph, return_elements=[modified_output_node_name])\n",
    "    print(output_node)\n",
    "    \n",
    "    tensor_names = [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    print(tensor_names)\n",
    "    \n",
    "    # create random data for warming up\n",
    "    warmup_sample = np.random.randint(low=0, high=256, size=(batch_size, n_height, n_width, n_channels))\n",
    "    warmup_sample = warmup_sample.astype(np.float32)\n",
    "    _ = sess.run(output_node, feed_dict={modified_input_node_name: warmup_sample})\n",
    "\n",
    "    # Run\n",
    "    timings = []\n",
    "    for _ in range(n_inferences):\n",
    "        # create random data for testing - TODO replace this with test data\n",
    "        test_data = np.random.randint(low=0, high=256, size=(batch_size, n_height, n_width, n_channels))\n",
    "        test_data = test_data.astype(np.float32)\n",
    "        \n",
    "        # time inference\n",
    "        start = time.time()\n",
    "        results_tf_trt = sess.run(output_node, feed_dict={modified_input_node_name: test_data})\n",
    "        end = time.time()\n",
    "        timings.append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00648188591003418, 0.004743337631225586, 0.004198789596557617, 0.004148006439208984, 0.004114389419555664, 0.004171133041381836, 0.004261970520019531, 0.003983259201049805, 0.004075527191162109, 0.00412297248840332, 0.00400543212890625, 0.004207611083984375, 0.0040547847747802734, 0.004126787185668945, 0.004075050354003906, 0.0042057037353515625, 0.004080057144165039, 0.0040514469146728516, 0.004174709320068359, 0.004047393798828125, 0.004068851470947266, 0.0041429996490478516, 0.004045724868774414, 0.003919839859008789, 0.00420379638671875, 0.00415349006652832, 0.003972291946411133, 0.004252433776855469, 0.004261493682861328, 0.004236698150634766, 0.0039038658142089844, 0.0034399032592773438, 0.0033180713653564453, 0.003351449966430664, 0.0032715797424316406, 0.0038657188415527344, 0.003404378890991211, 0.0034673213958740234, 0.003287792205810547, 0.003895282745361328, 0.0033206939697265625, 0.003272533416748047, 0.003278493881225586, 0.003315448760986328, 0.003345966339111328, 0.0032525062561035156, 0.003387928009033203, 0.003239870071411133, 0.0032868385314941406, 0.0032660961151123047, 0.003209352493286133, 0.003186941146850586, 0.0033118724822998047, 0.0031952857971191406, 0.0034096240997314453, 0.0033905506134033203, 0.003199338912963867, 0.0032367706298828125, 0.0034089088439941406, 0.0033609867095947266, 0.0032019615173339844, 0.003303050994873047, 0.0031468868255615234, 0.003381490707397461, 0.0033173561096191406, 0.0032911300659179688, 0.0031938552856445312, 0.0033507347106933594, 0.003299236297607422, 0.0032274723052978516, 0.003283977508544922, 0.003289461135864258, 0.003216266632080078, 0.0034058094024658203, 0.003278017044067383, 0.0031800270080566406, 0.0032782554626464844, 0.0032656192779541016, 0.003245830535888672, 0.003274202346801758, 0.003153562545776367, 0.0033473968505859375, 0.0032863616943359375, 0.003189563751220703, 0.003335714340209961, 0.003375530242919922, 0.0033037662506103516, 0.003326416015625, 0.003258228302001953, 0.0033512115478515625, 0.003195524215698242, 0.003177165985107422, 0.0033524036407470703, 0.003450155258178711, 0.003163576126098633, 0.0030930042266845703, 0.0032355785369873047, 0.003170490264892578, 0.003251791000366211, 0.0031309127807617188, 0.0032808780670166016, 0.0032339096069335938, 0.0031599998474121094, 0.003263235092163086, 0.0032684803009033203, 0.003152608871459961, 0.0032706260681152344, 0.003220081329345703, 0.003260374069213867, 0.0034842491149902344, 0.0032787322998046875, 0.0032677650451660156, 0.0033104419708251953, 0.003336191177368164, 0.003460407257080078, 0.003243684768676758, 0.0034132003784179688, 0.0032393932342529297, 0.003490924835205078, 0.0034780502319335938, 0.0031671524047851562, 0.0032558441162109375, 0.0031805038452148438, 0.00310516357421875, 0.0031630992889404297, 0.0034542083740234375, 0.0032019615173339844, 0.003252267837524414, 0.003200531005859375, 0.0033006668090820312, 0.003257274627685547, 0.003481626510620117, 0.0031859874725341797, 0.0031883716583251953, 0.0035016536712646484, 0.0033605098724365234, 0.0033130645751953125, 0.0034236907958984375, 0.0032851696014404297, 0.003317117691040039, 0.00321197509765625, 0.003271341323852539, 0.003304719924926758, 0.0033271312713623047, 0.0033330917358398438, 0.0033521652221679688, 0.003176450729370117, 0.0032885074615478516, 0.003232717514038086, 0.0032639503479003906, 0.0032651424407958984, 0.0033235549926757812, 0.0031409263610839844, 0.003423452377319336, 0.003339052200317383, 0.003219127655029297, 0.003327608108520508, 0.003405332565307617, 0.003193378448486328, 0.0032346248626708984, 0.003358602523803711, 0.003310680389404297, 0.0032405853271484375, 0.003305673599243164, 0.003247976303100586, 0.0032062530517578125, 0.003361940383911133, 0.0033593177795410156, 0.0032689571380615234, 0.003464937210083008, 0.0034296512603759766, 0.0032367706298828125, 0.0033431053161621094, 0.003426074981689453, 0.003227710723876953, 0.003253936767578125, 0.003171205520629883, 0.003265857696533203, 0.003278493881225586, 0.003391265869140625, 0.0033698081970214844, 0.003189563751220703, 0.003268718719482422, 0.0032215118408203125, 0.0033180713653564453, 0.003223419189453125, 0.003296375274658203, 0.0032393932342529297, 0.0033304691314697266, 0.0033369064331054688, 0.003358125686645508, 0.0032651424407958984, 0.0032799243927001953, 0.0032253265380859375, 0.0033690929412841797, 0.003276348114013672, 0.0030837059020996094, 0.003454446792602539, 0.0032989978790283203, 0.0031561851501464844, 0.0032002925872802734, 0.0032155513763427734, 0.0033235549926757812, 0.0032792091369628906, 0.0033426284790039062, 0.0031397342681884766, 0.003208160400390625, 0.0034215450286865234, 0.0031571388244628906, 0.0034117698669433594, 0.0032999515533447266, 0.0031769275665283203, 0.003231525421142578, 0.003153085708618164, 0.0033042430877685547, 0.003221273422241211, 0.0031926631927490234, 0.003175497055053711, 0.003239154815673828, 0.003400564193725586, 0.003153085708618164, 0.0033979415893554688, 0.0033140182495117188, 0.0030722618103027344, 0.0032880306243896484, 0.0030755996704101562, 0.0031795501708984375, 0.0033729076385498047, 0.0033974647521972656, 0.003338336944580078, 0.003310680389404297, 0.0031766891479492188, 0.003242969512939453, 0.0033330917358398438, 0.0032896995544433594, 0.003400087356567383, 0.003244161605834961, 0.003298521041870117, 0.003286123275756836, 0.003261566162109375, 0.0031783580780029297, 0.0033402442932128906, 0.00342559814453125, 0.0032172203063964844, 0.0033516883850097656, 0.0032095909118652344, 0.003386974334716797, 0.003348827362060547, 0.0032699108123779297, 0.003168821334838867, 0.0032715797424316406, 0.0033142566680908203, 0.0031583309173583984, 0.0032722949981689453, 0.003148794174194336, 0.0032546520233154297, 0.0032525062561035156, 0.0032317638397216797, 0.0032274723052978516, 0.003147602081298828, 0.003133058547973633, 0.0032036304473876953, 0.003148317337036133, 0.003201723098754883, 0.003123044967651367, 0.0032553672790527344, 0.0031614303588867188, 0.003192901611328125, 0.0034105777740478516, 0.0034296512603759766, 0.0032036304473876953, 0.003475666046142578, 0.0034809112548828125, 0.0032885074615478516, 0.0033037662506103516, 0.003213644027709961, 0.0033223628997802734, 0.003260374069213867, 0.0032465457916259766, 0.0032062530517578125, 0.0032367706298828125, 0.00322723388671875, 0.0032203197479248047, 0.0031936168670654297, 0.0033125877380371094, 0.0033805370330810547, 0.0031931400299072266, 0.003218412399291992, 0.00331878662109375, 0.003242969512939453, 0.0032851696014404297, 0.003215312957763672, 0.0033066272735595703, 0.003334522247314453, 0.003190755844116211, 0.003194093704223633, 0.0032236576080322266, 0.0032448768615722656, 0.003328084945678711, 0.003213644027709961, 0.003190755844116211, 0.003197193145751953, 0.0032546520233154297, 0.0032525062561035156, 0.00322723388671875, 0.0031719207763671875, 0.003490447998046875, 0.003284931182861328, 0.0034024715423583984, 0.003316640853881836, 0.003258228302001953, 0.0033218860626220703, 0.003211498260498047, 0.003267049789428711, 0.0032613277435302734, 0.0032165050506591797, 0.0032188892364501953, 0.0033795833587646484, 0.003184795379638672, 0.0032012462615966797, 0.0033774375915527344, 0.0033490657806396484, 0.003345012664794922, 0.0032329559326171875, 0.003391265869140625, 0.0031974315643310547, 0.0032525062561035156, 0.0031888484954833984, 0.0031921863555908203, 0.0033273696899414062, 0.0031538009643554688, 0.003256559371948242, 0.003209352493286133, 0.003261566162109375, 0.003275156021118164, 0.003214120864868164, 0.003289461135864258, 0.003209352493286133, 0.003154754638671875, 0.0032241344451904297, 0.0033969879150390625, 0.00323486328125, 0.003305196762084961, 0.0032236576080322266, 0.003309011459350586, 0.0032110214233398438, 0.0031838417053222656, 0.003407001495361328, 0.0032417774200439453, 0.003170490264892578, 0.0034172534942626953, 0.00341033935546875, 0.0032677650451660156, 0.0034024715423583984, 0.003427267074584961, 0.003240346908569336, 0.003362417221069336, 0.0032477378845214844, 0.003230571746826172, 0.003288745880126953, 0.0032024383544921875, 0.0032999515533447266, 0.003343343734741211, 0.003294229507446289, 0.0032041072845458984, 0.003228902816772461, 0.0035562515258789062, 0.0033216476440429688, 0.0034787654876708984, 0.0033044815063476562, 0.0033731460571289062, 0.0033354759216308594, 0.003316640853881836, 0.003410816192626953, 0.0032529830932617188, 0.003209352493286133, 0.0034313201904296875, 0.0034279823303222656, 0.003412008285522461, 0.003233671188354492, 0.0032806396484375, 0.0034248828887939453, 0.0033369064331054688, 0.0033042430877685547, 0.0032362937927246094, 0.0032958984375, 0.0033202171325683594, 0.0033576488494873047, 0.0033648014068603516, 0.0033006668090820312, 0.0032677650451660156, 0.003354787826538086, 0.003412485122680664, 0.003276348114013672, 0.0033140182495117188, 0.0032434463500976562, 0.0034728050231933594, 0.0033845901489257812, 0.003507375717163086, 0.0033788681030273438, 0.0032188892364501953, 0.003314971923828125, 0.003273487091064453, 0.0032188892364501953, 0.003305196762084961, 0.0033905506134033203, 0.0033152103424072266, 0.003276348114013672, 0.0032646656036376953, 0.003214120864868164, 0.0032434463500976562, 0.003379344940185547, 0.0032639503479003906, 0.003226041793823242, 0.0032868385314941406, 0.003254413604736328, 0.003259420394897461, 0.003220796585083008, 0.003252744674682617, 0.0031938552856445312, 0.0032584667205810547, 0.0032057762145996094, 0.003269672393798828, 0.0032885074615478516, 0.003267049789428711, 0.0032999515533447266, 0.003238201141357422, 0.0034041404724121094, 0.0032384395599365234, 0.003378629684448242, 0.003271341323852539, 0.0032503604888916016, 0.003397226333618164, 0.0032701492309570312, 0.0034210681915283203, 0.003293752670288086, 0.003247499465942383, 0.0032634735107421875, 0.0033943653106689453, 0.0032978057861328125, 0.003258228302001953, 0.003280162811279297, 0.003257274627685547, 0.0032911300659179688, 0.0032711029052734375, 0.0033054351806640625, 0.003315448760986328, 0.0032792091369628906, 0.0033867359161376953, 0.003489255905151367, 0.003303050994873047, 0.003272533416748047, 0.0034508705139160156, 0.0033082962036132812, 0.0033240318298339844, 0.0032701492309570312, 0.003297567367553711, 0.0032062530517578125, 0.003347158432006836, 0.003272533416748047, 0.003366708755493164, 0.003283262252807617, 0.0034241676330566406, 0.0033597946166992188, 0.0032813549041748047, 0.003313302993774414, 0.003268003463745117, 0.003279447555541992, 0.003283977508544922, 0.0032470226287841797, 0.003431558609008789, 0.003316164016723633, 0.0033783912658691406, 0.003332376480102539, 0.0033812522888183594, 0.003307342529296875, 0.0033087730407714844, 0.0034990310668945312, 0.003374338150024414, 0.003276348114013672, 0.003386974334716797, 0.003365039825439453, 0.0032491683959960938, 0.003367900848388672, 0.003213644027709961, 0.003216266632080078, 0.0033779144287109375, 0.0033414363861083984, 0.003244161605834961, 0.003340005874633789, 0.0034956932067871094, 0.003347158432006836, 0.0033342838287353516, 0.003301382064819336, 0.0033426284790039062, 0.0032935142517089844, 0.003300905227661133, 0.0034160614013671875, 0.0033109188079833984, 0.003281116485595703, 0.0034580230712890625, 0.003400564193725586, 0.003369569778442383, 0.003210306167602539, 0.0034034252166748047, 0.0033893585205078125, 0.003360271453857422, 0.0034351348876953125, 0.003281116485595703, 0.0033111572265625, 0.003282308578491211, 0.00330352783203125, 0.0032951831817626953, 0.0032863616943359375, 0.0032911300659179688, 0.0032312870025634766, 0.003427267074584961, 0.0032944679260253906, 0.0034089088439941406, 0.0032994747161865234, 0.003337383270263672, 0.00335693359375, 0.003251791000366211, 0.003451108932495117, 0.003306150436401367, 0.003287792205810547, 0.003473043441772461, 0.0033140182495117188, 0.0032110214233398438, 0.003318309783935547, 0.003316640853881836, 0.0033648014068603516, 0.003223896026611328, 0.003393411636352539, 0.003320455551147461, 0.00325775146484375, 0.003253936767578125, 0.0032987594604492188, 0.003385305404663086, 0.0034270286560058594, 0.003226757049560547, 0.003301858901977539, 0.0033922195434570312, 0.0032753944396972656, 0.003240346908569336, 0.0033500194549560547, 0.0035097599029541016, 0.003391265869140625, 0.0033195018768310547, 0.0034847259521484375, 0.0032727718353271484, 0.0033562183380126953, 0.003309011459350586, 0.0033118724822998047, 0.003335237503051758, 0.003426790237426758, 0.0033416748046875, 0.0033135414123535156, 0.0033130645751953125, 0.003307819366455078, 0.0032806396484375, 0.003306865692138672, 0.0033550262451171875, 0.0034945011138916016, 0.0032806396484375, 0.0032548904418945312, 0.003249645233154297, 0.003277301788330078, 0.003312349319458008, 0.0034132003784179688, 0.0032041072845458984, 0.003247976303100586, 0.0032660961151123047, 0.003266572952270508, 0.003194570541381836, 0.0032296180725097656, 0.0033273696899414062, 0.003258228302001953, 0.0032644271850585938, 0.0032503604888916016, 0.0032737255096435547, 0.0032608509063720703, 0.0032389163970947266, 0.003282785415649414, 0.003213167190551758, 0.003515481948852539, 0.0033941268920898438, 0.0034694671630859375, 0.003326416015625, 0.0031480789184570312, 0.003556966781616211, 0.003438234329223633, 0.0034821033477783203, 0.003231048583984375, 0.0033714771270751953, 0.0032384395599365234, 0.0033206939697265625, 0.003368854522705078, 0.003397226333618164, 0.0032753944396972656, 0.0034759044647216797, 0.0033164024353027344, 0.0032486915588378906, 0.0032482147216796875, 0.0033075809478759766, 0.0032722949981689453, 0.003263235092163086, 0.0033843517303466797, 0.003325223922729492, 0.003254413604736328, 0.003340482711791992, 0.0034024715423583984, 0.0034401416778564453, 0.003421783447265625, 0.003396749496459961, 0.003304719924926758, 0.0034706592559814453, 0.0032820701599121094, 0.003337383270263672, 0.003255128860473633, 0.0034873485565185547, 0.0034117698669433594, 0.003422260284423828, 0.003401041030883789, 0.0034089088439941406, 0.003204822540283203, 0.0033414363861083984, 0.0032389163970947266, 0.003372669219970703, 0.003290414810180664, 0.003253936767578125, 0.0032753944396972656, 0.0032999515533447266, 0.0032927989959716797, 0.0034914016723632812, 0.0034117698669433594, 0.0033066272735595703, 0.003316164016723633, 0.0033943653106689453, 0.0033936500549316406, 0.003284931182861328, 0.003310680389404297, 0.0033822059631347656, 0.0032587051391601562, 0.003186464309692383, 0.0032765865325927734, 0.003360271453857422, 0.003114461898803711, 0.003281116485595703, 0.0031898021697998047, 0.003294229507446289, 0.003166675567626953, 0.0033791065216064453, 0.0033843517303466797, 0.0032014846801757812, 0.003343343734741211, 0.003084421157836914, 0.0033457279205322266, 0.0032939910888671875, 0.0031502246856689453, 0.003050565719604492, 0.0030379295349121094, 0.003027200698852539, 0.003057718276977539, 0.0030138492584228516, 0.0031707286834716797, 0.002990245819091797, 0.0031762123107910156, 0.0031599998474121094, 0.003181934356689453, 0.0030558109283447266, 0.0030732154846191406, 0.0032863616943359375, 0.0030832290649414062, 0.003057718276977539, 0.0030760765075683594, 0.0029420852661132812, 0.0032007694244384766, 0.0032241344451904297, 0.003055572509765625, 0.00307464599609375, 0.003024578094482422, 0.0029726028442382812, 0.0030639171600341797, 0.0030159950256347656, 0.002997875213623047, 0.002996683120727539, 0.003101825714111328, 0.0031821727752685547, 0.0032167434692382812, 0.003046751022338867, 0.003031015396118164, 0.0032019615173339844, 0.0033211708068847656, 0.0032095909118652344, 0.0031371116638183594, 0.0031549930572509766, 0.003261089324951172, 0.003052949905395508, 0.0030372142791748047, 0.0030946731567382812, 0.003030061721801758, 0.003037691116333008, 0.002981424331665039, 0.0030562877655029297, 0.003031492233276367, 0.0030481815338134766, 0.003179311752319336, 0.0033354759216308594, 0.0030815601348876953, 0.0032362937927246094, 0.0029745101928710938, 0.0031037330627441406, 0.0030515193939208984, 0.003549814224243164, 0.0031774044036865234, 0.003095388412475586, 0.0031931400299072266, 0.0030601024627685547, 0.0033111572265625, 0.003210783004760742, 0.0031495094299316406, 0.003028392791748047, 0.0030546188354492188, 0.0030622482299804688, 0.0030460357666015625, 0.0030972957611083984, 0.003061056137084961, 0.003022909164428711, 0.002984285354614258, 0.0029823780059814453, 0.0030765533447265625, 0.0029916763305664062, 0.0031342506408691406, 0.0029900074005126953, 0.002958059310913086, 0.002972841262817383, 0.003003835678100586, 0.0029489994049072266, 0.003026247024536133, 0.0030210018157958984, 0.003069162368774414, 0.0031006336212158203, 0.003064870834350586, 0.0030679702758789062, 0.003019094467163086, 0.0030095577239990234, 0.0031061172485351562, 0.003087282180786133, 0.0031774044036865234, 0.0032613277435302734, 0.003228425979614258, 0.003075838088989258, 0.003503561019897461, 0.003047943115234375, 0.003221750259399414, 0.0031325817108154297, 0.003064393997192383, 0.003019094467163086, 0.003195524215698242, 0.003127574920654297, 0.0031859874725341797, 0.0030820369720458984, 0.003032684326171875, 0.0030426979064941406, 0.003072023391723633, 0.0030341148376464844, 0.003248929977416992, 0.0032694339752197266, 0.003080606460571289, 0.0029938220977783203, 0.003039836883544922, 0.003093719482421875, 0.0030701160430908203, 0.0030608177185058594, 0.003061532974243164, 0.0031805038452148438, 0.003190279006958008, 0.0030257701873779297, 0.0029802322387695312, 0.0030167102813720703, 0.0031151771545410156, 0.0030078887939453125, 0.0030660629272460938, 0.0030074119567871094, 0.0033419132232666016, 0.003050088882446289, 0.00331878662109375, 0.0029764175415039062, 0.003053903579711914, 0.003027200698852539, 0.003042459487915039, 0.0030159950256347656, 0.0032155513763427734, 0.003060579299926758, 0.0032587051391601562, 0.0030744075775146484, 0.0029234886169433594, 0.0030906200408935547, 0.0032618045806884766, 0.00301361083984375, 0.0030786991119384766, 0.0032050609588623047, 0.0030755996704101562, 0.0030536651611328125, 0.003266572952270508, 0.0030617713928222656, 0.003144979476928711, 0.003153562545776367, 0.003235340118408203, 0.0031824111938476562, 0.003130197525024414, 0.0033423900604248047, 0.0031270980834960938, 0.003045320510864258, 0.003022909164428711, 0.003192901611328125, 0.003136157989501953, 0.0031294822692871094, 0.0031423568725585938, 0.0030863285064697266, 0.003056049346923828, 0.003295421600341797, 0.0031006336212158203, 0.0030236244201660156, 0.0033593177795410156, 0.003023862838745117, 0.003130674362182617, 0.0032787322998046875, 0.0030493736267089844, 0.003262758255004883, 0.0030264854431152344, 0.0030133724212646484, 0.0031538009643554688, 0.003156900405883789, 0.003010988235473633, 0.003119945526123047, 0.0032205581665039062, 0.0030515193939208984, 0.003231525421142578, 0.003083944320678711, 0.0032334327697753906, 0.0030219554901123047, 0.002943754196166992, 0.003050565719604492, 0.003210306167602539, 0.003311634063720703, 0.0030798912048339844, 0.003034830093383789, 0.003214120864868164, 0.0029861927032470703, 0.0030548572540283203, 0.0032548904418945312, 0.003200531005859375, 0.003252744674682617, 0.003165721893310547, 0.0032835006713867188, 0.0031731128692626953, 0.0032625198364257812, 0.003285646438598633, 0.0032923221588134766, 0.003236532211303711, 0.0030393600463867188, 0.0031278133392333984, 0.0031251907348632812, 0.003252744674682617, 0.0032050609588623047, 0.0031876564025878906, 0.003135204315185547, 0.003112316131591797, 0.003190279006958008, 0.003074169158935547, 0.0033233165740966797, 0.0032873153686523438, 0.0030181407928466797, 0.0030241012573242188, 0.0030965805053710938, 0.003130674362182617, 0.003191709518432617, 0.0030364990234375, 0.0030329227447509766, 0.003052949905395508, 0.0030236244201660156, 0.003042936325073242, 0.003091573715209961, 0.003098726272583008, 0.0032753944396972656, 0.003166675567626953, 0.0030684471130371094, 0.0030748844146728516, 0.003219127655029297, 0.003081083297729492, 0.003240346908569336, 0.003049612045288086, 0.0031969547271728516, 0.003029346466064453, 0.0030260086059570312, 0.0030486583709716797, 0.002995729446411133, 0.0030803680419921875, 0.003020048141479492, 0.0029668807983398438, 0.0029659271240234375, 0.003003358840942383, 0.003008127212524414, 0.0030126571655273438, 0.002964019775390625, 0.0030243396759033203, 0.0030393600463867188, 0.003030061721801758, 0.003065347671508789, 0.0030832290649414062, 0.0030517578125, 0.0030701160430908203, 0.0029783248901367188, 0.003064393997192383, 0.0031478404998779297, 0.0029921531677246094, 0.002993345260620117, 0.0032951831817626953, 0.0032477378845214844, 0.003219127655029297, 0.0030660629272460938, 0.0031664371490478516, 0.0031235218048095703, 0.0032820701599121094, 0.0030059814453125, 0.003235340118408203, 0.0031003952026367188, 0.003016948699951172, 0.0031251907348632812, 0.0030493736267089844, 0.003271818161010742, 0.0032508373260498047, 0.003034830093383789, 0.003220796585083008, 0.0031354427337646484, 0.003209829330444336, 0.0031392574310302734, 0.0031621456146240234, 0.003278493881225586, 0.003171682357788086, 0.003220796585083008, 0.0030508041381835938, 0.003118276596069336, 0.003239154815673828, 0.0032587051391601562, 0.003243684768676758, 0.003316640853881836, 0.0030641555786132812, 0.003230571746826172, 0.0032112598419189453, 0.0032427310943603516, 0.0032262802124023438, 0.003184080123901367, 0.003099679946899414, 0.003124713897705078, 0.003157377243041992, 0.0032296180725097656, 0.0032372474670410156, 0.0031142234802246094, 0.003056049346923828, 0.0030634403228759766, 0.003014087677001953, 0.003005504608154297, 0.003108501434326172, 0.003112316131591797, 0.003059864044189453, 0.0032012462615966797, 0.0031366348266601562, 0.0030717849731445312, 0.003055572509765625, 0.003063201904296875, 0.003016948699951172, 0.003328084945678711, 0.0031309127807617188, 0.0031414031982421875, 0.0032258033752441406, 0.0032563209533691406, 0.0033028125762939453, 0.003072500228881836, 0.003117084503173828, 0.003137350082397461, 0.003220796585083008, 0.0030961036682128906, 0.0032999515533447266, 0.003161907196044922, 0.0032711029052734375, 0.0031299591064453125, 0.0030775070190429688, 0.0032112598419189453, 0.0029745101928710938, 0.0031480789184570312, 0.0031108856201171875, 0.0030541419982910156, 0.0031049251556396484, 0.003245830535888672, 0.0031986236572265625, 0.0031054019927978516, 0.003048419952392578]\n"
     ]
    }
   ],
   "source": [
    "# Check timings\n",
    "print(timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Inference Time: 3.26582407951355 seconds\n",
      "Number of Inferences: 1000\n",
      "Average Latency: 0.00326582407951355 +- 0.00022483372545169648 seconds\n",
      "Average Throughput w/ Batch Size 1: 306.2014289970425 examples per second\n"
     ]
    }
   ],
   "source": [
    "# Benchmark TF-TRT\n",
    "timings = np.asarray(timings)\n",
    "delta_tf_trt = np.sum(timings)\n",
    "average_latency_tf_trt = np.mean(timings)\n",
    "std_latency_tf_trt = np.std(timings)\n",
    "average_throughput_tf_trt = batch_size * (1 / average_latency_tf_trt)\n",
    "print('Total Inference Time: {} seconds'.format(delta_tf_trt))\n",
    "print('Number of Inferences: {}'.format(len(timings)))\n",
    "print('Average Latency: {} +- {} seconds'.format(average_latency_tf_trt, std_latency_tf_trt))\n",
    "print('Average Throughput w/ Batch Size {}: {} examples per second'.format(batch_size, average_throughput_tf_trt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results area identical: [[[-1.2761276e-09  6.0645770e-04  3.1037450e-02 -3.1643908e-02\n",
      "   -1.1389229e-08]]]\n",
      "Speedup of TF-TRT over TF is: 1.005096051872048x\n"
     ]
    }
   ],
   "source": [
    "# Compare results\n",
    "print('Results area identical:', results_tf - results_tf_trt)\n",
    "speedup = average_latency_tf / average_latency_tf_trt\n",
    "print('Speedup of TF-TRT over TF is: {}x'.format(speedup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
